{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b740da39-824e-4776-9fe7-95b6c8b1b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Alice|   200|\n",
      "|  Den|  NULL|\n",
      "|  Bob|   100|\n",
      "+-----+------+\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|  Den|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание DataFrame и использование join\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import avg, max, asc, desc, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('schema') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'),(2, 'Bob'),(3, 'Den')], ['id', 'name'])\n",
    "df2 = spark.createDataFrame([(1, 2, 100), (2, 1, 200)], ['id', 'user_id', 'salary'])\n",
    "\n",
    "df_join = df1.join(df2, df1.id == df2.user_id, how='left').select(df1.name, df2.salary)\n",
    "df_join.show()\n",
    "\n",
    "df1.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020a1a21-6444-4f7e-bcd2-7676d1ba5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|  Den|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Запись данных в Parquet\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Parquet-HDFS') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/parquet'\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'),(2, 'Bob'),(3, 'Den')], ['id', 'name'])\n",
    "df1.write.format('parquet').mode('overwrite').save(path)\n",
    "\n",
    "df_parquet = spark.read.parquet(path)\n",
    "df_parquet.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524a875e-3efb-4628-8f95-0687d55c3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Запись данных в ORC\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ORC-HDFS') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Пример данных\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/orc'\n",
    "\n",
    "# Запись данных в ORC\n",
    "df.write.orc(path)\n",
    "\n",
    "# Чтение данных из ORC\n",
    "df_orc = spark.read.orc(path)\n",
    "df_orc.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea2c928-ea0f-45fe-a882-054667915897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Заголовок', 1), ('', 2), ('Какой-то', 1), ('текст.', 1), ('-', 2), ('Список', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных с использованием RDD\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'Read txt')\n",
    "\n",
    "rdd = sc.textFile('hdfs://hadoop-namenode:9000/user/jovyan/input/text.txt')\n",
    "\n",
    "words = rdd.flatMap(lambda line: line.split(' '))\n",
    "words_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(words_counts.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acb31293-7733-48a4-9f3b-a9d3a133c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|users|\n",
      "+-----+\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение json файла\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('read json') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"users\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "df_json = spark.read.schema(schema).json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/test.json\")\n",
    "\n",
    "df_json.show()\n",
    "\n",
    "# df_users = df.select(explode(col('users')).alias('user'))\n",
    "\n",
    "# df_final = df_users.select('user.name', 'user.age')\n",
    "# df_final.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f37386-11be-42dc-891b-e1dd94ea6282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|Name |Age|\n",
      "+-----+---+\n",
      "|Bob  |20 |\n",
      "|Ivan |30 |\n",
      "|Alice|15 |\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read csv') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option('header', 'true').csv('hdfs://hadoop-namenode:9000/user/jovyan/output_csv/')\n",
    "df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7036689-e76e-46ed-a536-206ffda95df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запись в csv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read csv') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [('Alice', 15), ('Bob', 20), ('Ivan', 30)]\n",
    "columns = ['Name', 'Age']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.mode('overwrite').option('header', 'true').csv('hdfs://hadoop-namenode:9000/user/jovyan/output_csv/')\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bdca94a-5275-4b88-9e71-61604d0d2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------+-----+\n",
      "|_corrupt_record| age| department| name|\n",
      "+---------------+----+-----------+-----+\n",
      "|              [|NULL|       NULL| NULL|\n",
      "|           NULL|  30|         HR| John|\n",
      "|           NULL|  25|    Finance|  Doe|\n",
      "|           NULL|  35|         HR| Jane|\n",
      "|           NULL|  40|    Finance| Mark|\n",
      "|           NULL|  23|Engineering|Smith|\n",
      "|              ]|NULL|       NULL| NULL|\n",
      "+---------------+----+-----------+-----+\n",
      "\n",
      "+---------------+---+----------+----+\n",
      "|_corrupt_record|age|department|name|\n",
      "+---------------+---+----------+----+\n",
      "|           NULL| 35|        HR|Jane|\n",
      "|           NULL| 40|   Finance|Mark|\n",
      "+---------------+---+----------+----+\n",
      "\n",
      "+-----------+-----+-------+\n",
      "| department|count|avg_age|\n",
      "+-----------+-----+-------+\n",
      "|Engineering|    1|   23.0|\n",
      "|         HR|    2|   32.5|\n",
      "|       NULL|    0|   NULL|\n",
      "|    Finance|    2|   32.5|\n",
      "+-----------+-----+-------+\n",
      "\n",
      "+-----------+-----+-------+\n",
      "| department|count|avg_age|\n",
      "+-----------+-----+-------+\n",
      "|         HR|    2|   32.5|\n",
      "|    Finance|    2|   32.5|\n",
      "|Engineering|    1|   23.0|\n",
      "|       NULL|    0|   NULL|\n",
      "+-----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read json sql') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/input/test.json'\n",
    "\n",
    "df = spark.read.json(path)\n",
    "df.show()\n",
    "\n",
    "filtered_df = df.filter(col('age') > 30)\n",
    "filtered_df.show()\n",
    "\n",
    "grouped_df = df.groupBy('department').agg({\"age\": \"avg\", \"name\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"avg(age)\", \"avg_age\") \\\n",
    "    .withColumnRenamed(\"count(name)\", \"count\")\n",
    "grouped_df.show()\n",
    "\n",
    "sorted_df = grouped_df.orderBy(col(\"count\").desc())\n",
    "sorted_df.show()\n",
    "\n",
    "sorted_df.write.csv(\"output.csv\", header=True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287da6c2-42b4-4327-90c7-77bacc668094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+\n",
      "|age|department_id| name|\n",
      "+---+-------------+-----+\n",
      "| 30|            1| John|\n",
      "| 25|            2|  Doe|\n",
      "| 35|            1| Jane|\n",
      "| 40|            2| Mark|\n",
      "| 23|            3|Smith|\n",
      "+---+-------------+-----+\n",
      "\n",
      "+---------------+---+\n",
      "|department_name| id|\n",
      "+---------------+---+\n",
      "|             HR|  1|\n",
      "|        Finance|  2|\n",
      "|    Engineering|  3|\n",
      "+---------------+---+\n",
      "\n",
      "+-----+---+---------------+\n",
      "| name|age|department_name|\n",
      "+-----+---+---------------+\n",
      "| John| 30|             HR|\n",
      "|  Doe| 25|        Finance|\n",
      "| Jane| 35|             HR|\n",
      "| Mark| 40|        Finance|\n",
      "|Smith| 23|    Engineering|\n",
      "+-----+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SQL') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "people_df = spark.read.json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/people.json\")\n",
    "departments_df = spark.read.json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/depatments.json\")\n",
    "\n",
    "people_df.show()\n",
    "departments_df.show()\n",
    "\n",
    "people_df.createOrReplaceTempView('people')\n",
    "departments_df.createOrReplaceTempView('departments')\n",
    "\n",
    "join_df = spark.sql(\"\"\"\n",
    "SELECT p.name, p.age, d.department_name\n",
    "FROM people p\n",
    "JOIN departments d ON p.department_id = d.id\n",
    "\"\"\")\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9917da-db63-455f-8b9b-79a31927147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+--------+----------+\n",
      "| id|   name|position|  salary| hire_date|\n",
      "+---+-------+--------+--------+----------+\n",
      "|  1|  Alice|Engineer|75000.00|2021-06-15|\n",
      "|  2|    Bob| Manager|90000.00|2020-05-01|\n",
      "|  3|Charlie|      HR|60000.00|2019-04-12|\n",
      "+---+-------+--------+--------+----------+\n",
      "\n",
      "+---+-----+--------+--------+----------+\n",
      "| id| name|position|  salary| hire_date|\n",
      "+---+-----+--------+--------+----------+\n",
      "|  1|Alice|Engineer|75000.00|2021-06-15|\n",
      "|  2|  Bob| Manager|90000.00|2020-05-01|\n",
      "+---+-----+--------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Подключение к PostgreSQL\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PySpark PostgreSQL Connection') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .config('spark.jars', 'postgresql-42.7.3.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = 'jdbc:postgresql://postgres:5432/sparkdb'\n",
    "properties = {\n",
    "    'user': 'sparkuser',\n",
    "    'password': 'sparkpass',\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"employees\", properties=properties)\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView('empl')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select *\n",
    "    from empl e\n",
    "    where e.salary >= 65000\n",
    "\"\"\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2061d58-aafb-4ac4-804b-962e2b63c695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+\n",
      "|   name|position|salary| hire_date|\n",
      "+-------+--------+------+----------+\n",
      "|  Alice|Engineer| 75000|2021-06-15|\n",
      "|    Bob| Manager| 90000|2020-05-01|\n",
      "|Charlie|      HR| 60000|2019-04-12|\n",
      "|  Diana|   Sales| 50000|2018-01-25|\n",
      "+-------+--------+------+----------+\n",
      "\n",
      "Добавление новых строк завершено\n"
     ]
    }
   ],
   "source": [
    "# Запись в PostgreSQL\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PySpark PostgreSQL Connection') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .config('spark.jars', 'postgresql-42.7.3.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", \"Engineer\", 75000, \"2021-06-15\"),\n",
    "    (\"Bob\", \"Manager\", 90000, \"2020-05-01\"),\n",
    "    (\"Charlie\", \"HR\", 60000, \"2019-04-12\"),\n",
    "    (\"Diana\", \"Sales\", 50000, \"2018-01-25\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"position\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)  # сначала строка\n",
    "])\n",
    "\n",
    "# columns = ['name', 'position', 'salary', 'hire_date']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"hire_date\", to_date(df.hire_date, \"yyyy-MM-dd\"))\n",
    "df.show()\n",
    "\n",
    "url = 'jdbc:postgresql://postgres:5432/sparkdb'\n",
    "properties = {\n",
    "    'user': 'sparkuser',\n",
    "    'password': 'sparkpass',\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "\n",
    "df.write.jdbc(\n",
    "    url=url,\n",
    "    table=\"employees_2\",\n",
    "    mode=\"append\",\n",
    "    properties=properties\n",
    ")\n",
    "\n",
    "print('Добавление новых строк завершено')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e918b-9d5b-4aab-b41d-40c3585280f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
