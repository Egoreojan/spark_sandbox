{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b740da39-824e-4776-9fe7-95b6c8b1b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|Alice|   200|\n",
      "|  Den|  NULL|\n",
      "|  Bob|   100|\n",
      "+-----+------+\n",
      "\n",
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|  Den|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание DataFrame и использование join\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import avg, max, asc, desc, col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('schema') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'),(2, 'Bob'),(3, 'Den')], ['id', 'name'])\n",
    "df2 = spark.createDataFrame([(1, 2, 100), (2, 1, 200)], ['id', 'user_id', 'salary'])\n",
    "\n",
    "df_join = df1.join(df2, df1.id == df2.user_id, how='left').select(df1.name, df2.salary)\n",
    "df_join.show()\n",
    "\n",
    "df1.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020a1a21-6444-4f7e-bcd2-7676d1ba5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "|  3|  Den|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Запись данных в Parquet\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Parquet-HDFS') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/parquet'\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'),(2, 'Bob'),(3, 'Den')], ['id', 'name'])\n",
    "df1.write.format('parquet').mode('overwrite').save(path)\n",
    "\n",
    "df_parquet = spark.read.parquet(path)\n",
    "df_parquet.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524a875e-3efb-4628-8f95-0687d55c3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Cathy| 29|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Запись данных в ORC\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('ORC-HDFS') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Пример данных\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/orc'\n",
    "\n",
    "# Запись данных в ORC\n",
    "df.write.orc(path)\n",
    "\n",
    "# Чтение данных из ORC\n",
    "df_orc = spark.read.orc(path)\n",
    "df_orc.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea2c928-ea0f-45fe-a882-054667915897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Заголовок', 1), ('', 2), ('Какой-то', 1), ('текст.', 1), ('-', 2), ('Список', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Чтение данных с использованием RDD\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'Read txt')\n",
    "\n",
    "rdd = sc.textFile('hdfs://hadoop-namenode:9000/user/jovyan/input/text.txt')\n",
    "\n",
    "words = rdd.flatMap(lambda line: line.split(' '))\n",
    "words_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(words_counts.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acb31293-7733-48a4-9f3b-a9d3a133c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|users|\n",
      "+-----+\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "| NULL|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение json файла\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('read json') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"users\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"age\", IntegerType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "df_json = spark.read.schema(schema).json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/test.json\")\n",
    "\n",
    "df_json.show()\n",
    "\n",
    "# df_users = df.select(explode(col('users')).alias('user'))\n",
    "\n",
    "# df_final = df_users.select('user.name', 'user.age')\n",
    "# df_final.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2f37386-11be-42dc-891b-e1dd94ea6282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|Name |Age|\n",
      "+-----+---+\n",
      "|Bob  |20 |\n",
      "|Ivan |30 |\n",
      "|Alice|15 |\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read csv') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option('header', 'true').csv('hdfs://hadoop-namenode:9000/user/jovyan/output_csv/')\n",
    "df.show(truncate=False)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7036689-e76e-46ed-a536-206ffda95df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запись в csv\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read csv') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [('Alice', 15), ('Bob', 20), ('Ivan', 30)]\n",
    "columns = ['Name', 'Age']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.mode('overwrite').option('header', 'true').csv('hdfs://hadoop-namenode:9000/user/jovyan/output_csv/')\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bdca94a-5275-4b88-9e71-61604d0d2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------+-----+\n",
      "|_corrupt_record| age| department| name|\n",
      "+---------------+----+-----------+-----+\n",
      "|              [|NULL|       NULL| NULL|\n",
      "|           NULL|  30|         HR| John|\n",
      "|           NULL|  25|    Finance|  Doe|\n",
      "|           NULL|  35|         HR| Jane|\n",
      "|           NULL|  40|    Finance| Mark|\n",
      "|           NULL|  23|Engineering|Smith|\n",
      "|              ]|NULL|       NULL| NULL|\n",
      "+---------------+----+-----------+-----+\n",
      "\n",
      "+---------------+---+----------+----+\n",
      "|_corrupt_record|age|department|name|\n",
      "+---------------+---+----------+----+\n",
      "|           NULL| 35|        HR|Jane|\n",
      "|           NULL| 40|   Finance|Mark|\n",
      "+---------------+---+----------+----+\n",
      "\n",
      "+-----------+-----+-------+\n",
      "| department|count|avg_age|\n",
      "+-----------+-----+-------+\n",
      "|Engineering|    1|   23.0|\n",
      "|         HR|    2|   32.5|\n",
      "|       NULL|    0|   NULL|\n",
      "|    Finance|    2|   32.5|\n",
      "+-----------+-----+-------+\n",
      "\n",
      "+-----------+-----+-------+\n",
      "| department|count|avg_age|\n",
      "+-----------+-----+-------+\n",
      "|         HR|    2|   32.5|\n",
      "|    Finance|    2|   32.5|\n",
      "|Engineering|    1|   23.0|\n",
      "|       NULL|    0|   NULL|\n",
      "+-----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Чтение json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Read json sql') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "path = 'hdfs://hadoop-namenode:9000/user/jovyan/input/test.json'\n",
    "\n",
    "df = spark.read.json(path)\n",
    "df.show()\n",
    "\n",
    "filtered_df = df.filter(col('age') > 30)\n",
    "filtered_df.show()\n",
    "\n",
    "grouped_df = df.groupBy('department').agg({\"age\": \"avg\", \"name\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"avg(age)\", \"avg_age\") \\\n",
    "    .withColumnRenamed(\"count(name)\", \"count\")\n",
    "grouped_df.show()\n",
    "\n",
    "sorted_df = grouped_df.orderBy(col(\"count\").desc())\n",
    "sorted_df.show()\n",
    "\n",
    "sorted_df.write.csv(\"output.csv\", header=True)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "287da6c2-42b4-4327-90c7-77bacc668094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----+\n",
      "|age|department_id| name|\n",
      "+---+-------------+-----+\n",
      "| 30|            1| John|\n",
      "| 25|            2|  Doe|\n",
      "| 35|            1| Jane|\n",
      "| 40|            2| Mark|\n",
      "| 23|            3|Smith|\n",
      "+---+-------------+-----+\n",
      "\n",
      "+---------------+---+\n",
      "|department_name| id|\n",
      "+---------------+---+\n",
      "|             HR|  1|\n",
      "|        Finance|  2|\n",
      "|    Engineering|  3|\n",
      "+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('SQL') \\\n",
    "    .config('spark.master', 'spark://spark-master:7077') \\\n",
    "    .config('spark.hadoop.fs.defaultFS', 'hdfs://hadoop-namenode:9000') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "people_df = spark.read.json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/people.json\")\n",
    "departments_df = spark.read.json(\"hdfs://hadoop-namenode:9000/user/jovyan/input/depatments.json\")\n",
    "\n",
    "people_df.show()\n",
    "departments_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9917da-db63-455f-8b9b-79a31927147c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
